{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama PDF RAG Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Jupyter-specific imports\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set environment variable for protobuf\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF loaded successfully: HRPolicy_Manual2023.pdf\n"
     ]
    }
   ],
   "source": [
    "# Load PDF\n",
    "local_path = \"HRPolicy_Manual2023.pdf\"\n",
    "if local_path:\n",
    "    loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "    data = loader.load()\n",
    "    print(f\"PDF loaded successfully: {local_path}\")\n",
    "else:\n",
    "    print(\"Upload a PDF file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 512 chunks\n"
     ]
    }
   ],
   "source": [
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "print(f\"Text split into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "    collection_name=\"local-rag\"\n",
    ")\n",
    "print(\"Vector database created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up LLM and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM and retrieval\n",
    "local_model = \"llama3.2\" \n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query prompt template\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate 2\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "# Set up retriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt template\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_pdf(question):\n",
    "    \"\"\"\n",
    "    Chat with the PDF using the RAG chain.\n",
    "    \"\"\"\n",
    "    return display(Markdown(chain.invoke(question)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, here's an explanation of the structure and approval process for different types of leave at IIMA:\n",
       "\n",
       "1. **Half-Pay Leave**: The policy doesn't explicitly define \"earned\" or regular leave in this context, but it mentions half-pay leave as a separate type. This leave can be taken on medical grounds, with no maximum limit.\n",
       "2. **Casual Leave**: Casual leave is not mentioned as one of the types of leave that requires approval or has specific rules governing its use.\n",
       "3. **Maternity/Paternity Leave**: Maternity leave is available for 180 days immediately after the date of adoption (not necessarily childbirth). Paternity leave may be granted, but it's not explicitly linked to a specific duration or process in this context.\n",
       "4. **Half-Pay Leave on Medical Grounds**: This type of leave can be taken without a medical certificate for up to 90 days during the entire service. If commuted leave is taken, twice the number of days availed should be debited in the half-pay leave account.\n",
       "5. **Commuted Leave**: Commuted leave can be taken up to:\n",
       "\t* 90 days during the entire service without a medical certificate for an approved course of study certified to be in the public interest.\n",
       "\t* 60 days without a medical certificate for a female employee with less than two living children on the adoption of a child less than one-year-old.\n",
       "6. **Child Adoption Leave**: This leave is available for 180 days immediately after the date of adoption, and during this period, the female employee will be paid leave salary equal to the pay drawn immediately before proceeding on leave.\n",
       "\n",
       "It's worth noting that the policy doesn't provide detailed information about the approval process for these leaves. However, it does mention that certain types of leave (like child adoption leave) require an authority competent to grant leave."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Example 1\n",
    "chat_with_pdf(\"Can you explain how the different types of leave (e.g., earned, casual, half-pay, maternity/paternity) are structured and approved at IIMA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Earned Leave (EL) is a type of leave that an employee can apply for, subject to approval by their head of department. The key characteristics of Earned Leave are:\n",
       "\n",
       "* It will be reduced by 1/10th of EOL availed and/or the period of * during the previous half year, subject to a maximum of 15 days.\n",
       "* The EL application must be submitted through ESS (Employee Self Service) to the head of the department for approval, at least 15 days prior to the start of the leave.\n",
       "* Credit for earned leave is allowed at a specific rate:\n",
       "\t+ For the half-year in which an employee is due to retire or resigns from service: two & half days per completed calendar month up to the date of retirement or resignation.\n",
       "\t+ For employees who are removed or dismissed from service: two & half days per completed calendar month up to the end of the calendar month preceding the calendar month in which they are removed or dismissed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 2\n",
    "chat_with_pdf(\"Explain Earned leave type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3\n",
    "# chat_with_pdf(\"How are the petroleum products classified?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4\n",
    "# chat_with_pdf(\"What should be the minimum headroom under vessels,pipes,cable racks,etc.?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5\n",
    "# chat_with_pdf(\"Where should the High Tension sub-stations be located? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6\n",
    "# chat_with_pdf(\"The Aggregate capacity of tanks located in one Dyked enclosure should not exceed? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7\n",
    "# chat_with_pdf(\"Where is an area deemed to be hazardous? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 8\n",
    "# chat_with_pdf(\"On what aspects should information be collected to prepare a layout? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 8\n",
    "# chat_with_pdf(\"Explain firewalls in layout of storage tanks. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "#chat_with_pdf(\"what should be the minimum slope of the return line from compressor back to reservoir?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2\n",
    "#hat_with_pdf(\"What are the major auxiliary equipments used in a compressor?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example 3\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m chat_with_pdf(\u001b[33m\"\u001b[39m\u001b[33mList the significant factors to be considered while deciding location of pumps?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mchat_with_pdf\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat_with_pdf\u001b[39m(question):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Chat with the PDF using the RAG chain.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m display(Markdown(chain.invoke(question)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3024\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3022\u001b[39m             \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m   3023\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3024\u001b[39m             \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[32m   3025\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3026\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:286\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    276\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    277\u001b[39m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m    281\u001b[39m     **kwargs: Any,\n\u001b[32m    282\u001b[39m ) -> BaseMessage:\n\u001b[32m    283\u001b[39m     config = ensure_config(config)\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    285\u001b[39m         ChatGeneration,\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m         \u001b[38;5;28mself\u001b[39m.generate_prompt(\n\u001b[32m    287\u001b[39m             [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    288\u001b[39m             stop=stop,\n\u001b[32m    289\u001b[39m             callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    290\u001b[39m             tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    291\u001b[39m             metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    292\u001b[39m             run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    293\u001b[39m             run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    294\u001b[39m             **kwargs,\n\u001b[32m    295\u001b[39m         ).generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    296\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:786\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    779\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    780\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[32m   (...)\u001b[39m\u001b[32m    783\u001b[39m     **kwargs: Any,\n\u001b[32m    784\u001b[39m ) -> LLMResult:\n\u001b[32m    785\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:643\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    642\u001b[39m             run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    644\u001b[39m flattened_outputs = [\n\u001b[32m    645\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    647\u001b[39m ]\n\u001b[32m    648\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:633\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    632\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m             \u001b[38;5;28mself\u001b[39m._generate_with_cache(\n\u001b[32m    634\u001b[39m                 m,\n\u001b[32m    635\u001b[39m                 stop=stop,\n\u001b[32m    636\u001b[39m                 run_manager=run_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    637\u001b[39m                 **kwargs,\n\u001b[32m    638\u001b[39m             )\n\u001b[32m    639\u001b[39m         )\n\u001b[32m    640\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    641\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:851\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    849\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    850\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m851\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(\n\u001b[32m    852\u001b[39m             messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m    853\u001b[39m         )\n\u001b[32m    854\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    855\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:644\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    638\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    639\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    642\u001b[39m     **kwargs: Any,\n\u001b[32m    643\u001b[39m ) -> ChatResult:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     final_chunk = \u001b[38;5;28mself\u001b[39m._chat_stream_with_aggregation(\n\u001b[32m    645\u001b[39m         messages, stop, run_manager, verbose=\u001b[38;5;28mself\u001b[39m.verbose, **kwargs\n\u001b[32m    646\u001b[39m     )\n\u001b[32m    647\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m    648\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m    649\u001b[39m         message=AIMessage(\n\u001b[32m    650\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m    654\u001b[39m         generation_info=generation_info,\n\u001b[32m    655\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:545\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    537\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    538\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    542\u001b[39m     **kwargs: Any,\n\u001b[32m    543\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    544\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_stream(messages, stop, **kwargs):\n\u001b[32m    546\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    547\u001b[39m             chunk = ChatGenerationChunk(\n\u001b[32m    548\u001b[39m                 message=AIMessageChunk(\n\u001b[32m    549\u001b[39m                     content=(\n\u001b[32m   (...)\u001b[39m\u001b[32m    562\u001b[39m                 ),\n\u001b[32m    563\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:527\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(\n\u001b[32m    518\u001b[39m         model=params[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    519\u001b[39m         messages=ollama_messages,\n\u001b[32m   (...)\u001b[39m\u001b[32m    524\u001b[39m         tools=kwargs[\u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    525\u001b[39m     )\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(\n\u001b[32m    528\u001b[39m         model=params[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    529\u001b[39m         messages=ollama_messages,\n\u001b[32m    530\u001b[39m         stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    531\u001b[39m         options=Options(**params[\u001b[33m\"\u001b[39m\u001b[33moptions\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m    532\u001b[39m         keep_alive=params[\u001b[33m\"\u001b[39m\u001b[33mkeep_alive\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    533\u001b[39m         \u001b[38;5;28mformat\u001b[39m=params[\u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    534\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/ollama/_client.py:80\u001b[39m, in \u001b[36mClient._stream\u001b[39m\u001b[34m(self, method, url, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_stream\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, **kwargs) -> Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.stream(method, url, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     82\u001b[39m       r.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/httpx/_client.py:880\u001b[39m, in \u001b[36mClient.stream\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    857\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    858\u001b[39m \u001b[33;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[32m    859\u001b[39m \u001b[33;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    865\u001b[39m \u001b[33;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[32m    866\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    867\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    868\u001b[39m     method=method,\n\u001b[32m    869\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m     extensions=extensions,\n\u001b[32m    879\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m880\u001b[39m response = \u001b[38;5;28mself\u001b[39m.send(\n\u001b[32m    881\u001b[39m     request=request,\n\u001b[32m    882\u001b[39m     auth=auth,\n\u001b[32m    883\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m    884\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    885\u001b[39m )\n\u001b[32m    886\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    887\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/httpx/_client.py:926\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    924\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m response = \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m    927\u001b[39m     request,\n\u001b[32m    928\u001b[39m     auth=auth,\n\u001b[32m    929\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m    930\u001b[39m     history=[],\n\u001b[32m    931\u001b[39m )\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/httpx/_client.py:954\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    951\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m     response = \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m    955\u001b[39m         request,\n\u001b[32m    956\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m    957\u001b[39m         history=history,\n\u001b[32m    958\u001b[39m     )\n\u001b[32m    959\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    960\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/httpx/_client.py:991\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    989\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m response = \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    993\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/httpx/_client.py:1027\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1023\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1024\u001b[39m     )\n\u001b[32m   1026\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     response = transport.handle_request(request)\n\u001b[32m   1029\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1031\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/httpx/_transports/default.py:236\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    223\u001b[39m req = httpcore.Request(\n\u001b[32m    224\u001b[39m     method=request.method,\n\u001b[32m    225\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    233\u001b[39m     extensions=request.extensions,\n\u001b[32m    234\u001b[39m )\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m._pool.handle_request(req)\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    241\u001b[39m     status_code=resp.status,\n\u001b[32m    242\u001b[39m     headers=resp.headers,\n\u001b[32m    243\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    244\u001b[39m     extensions=resp.extensions,\n\u001b[32m    245\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = connection.handle_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28mself\u001b[39m._receive_response_headers(**kwargs)\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/how/venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sock.recv(max_bytes)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Example 3\n",
    "chat_with_pdf(\"List the significant factors to be considered while deciding location of pumps?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the provided context, it is mentioned that \"Compressor may be electric driven or turbine driven (Gas/Steam). In case of electric driven compressors, it is necessary to have physical dimensions of motor and its cooling system if any to allocate space required in the layout for installation and maintenance.\"\n",
       "\n",
       "There is no specific information about the compressor configuration provided. However, further in the text:\n",
       "\n",
       "\"Grade mounted horizontal split case compressor may have increased downtime and other complications during a general maintenance as the suction and discharge piping may have to be dismantled to remove the casing.\"\n",
       "\n",
       "And also \n",
       "\n",
       "\"In Vertical split case type, the maintenance space required for removal of casing shall be provided at the side.\"\n",
       "\n",
       "It is mentioned that \"Grade mounted horizontal split case compressor\" has a configuration that requires:\n",
       "\n",
       "* Increased downtime\n",
       "* Suction and discharge piping dismantling during maintenance\n",
       "\n",
       "But no top suction or side discharge are explicitly described."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 4\n",
    "chat_with_pdf(\"Which compressor has a  a top suction, top discharge configuration or side suction side discharge configuration?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the provided context, the following API and/or ASME Standards are considered when designing piping:\n",
       "\n",
       "1. ASME B31.1 - Power Piping\n",
       "2. ASME B31.3 - Process Piping (most commonly applied)\n",
       "3. ASME B31.4 - Pipeline Transportation Systems for Liquids and Slurries\n",
       "4. ASME B31.8 - Gas Transmission and Distribution Piping Systems\n",
       "5. API RP-14E - Practice for Design and Installation of Offshore Production Platform Piping Systems (Applicable for Offshore applications)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 5\n",
    "chat_with_pdf(\"What API and/or ASME Standards are considered while piping is designed?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the provided context, block valves in pump suction lines shall generally be full port and shall be located upstream of any strainer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 6\n",
    "chat_with_pdf(\"where are the block valves in pump suction lines located?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the document, eccentric reducers are preferred at pump suction and should be installed \"TOP FLAT\" (flat on top) to minimize cavitation issues."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 7\n",
    "chat_with_pdf(\"How are the eccentric reducers installed to minimize cavitation??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the text, the piping should not obstruct the following:\n",
       "\n",
       "* Process flow requirements (specifically, gravity flow, no pockets, self-draining, slope, and relative elevation of equipment)\n",
       "* Safety gates\n",
       "* Handrails (there should be a clearance of at least 75mm between the outside of the handrail and the nearest object)\n",
       "\n",
       "In general, piping should not obstruct walkways, platforms, ramps, or floors, but the specific wording of \"obstruct\" may vary depending on the context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 8\n",
    "chat_with_pdf(\"What shall the piping not obstruct??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the provided text, while placing a strainer in the pipeline, the following considerations should be made:\n",
       "\n",
       "1. Strainers can be mounted in Horizontal or vertical runs, but correct orientation is required.\n",
       "2. Sufficient space should be allowed for removal of filter without having to remove additional equipment.\n",
       "3. Strainers should not be orientated so that solids can fall back into process line.\n",
       "4. Strainer should only be mounted in horizontal lines with cover facing upwards for access.\n",
       "\n",
       "Additionally, it's mentioned that In Line Basket Strainers are designed for installation in horizontal lines and should be oriented such that the basket is facing downwards to allow for easy cleaning."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 9\n",
    "chat_with_pdf(\"What should be considered while placing a strainer in the pipeline?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided text, here are some significant factors to consider when locating exchangers in a plant layout:\n",
       "\n",
       "1. **Accessibility and Space**: Ensure that the location of exchangers allows for easy access, maintenance, and repair.\n",
       "2. **Temperature and Pressure Drop**: Consider the temperature and pressure drop across the exchanger to ensure that it can handle the required flow rates and pressures.\n",
       "3. **Heat Transfer Area**: Choose a location with sufficient heat transfer area to achieve the desired heat exchange performance.\n",
       "4. **Corrosion Protection**: Select a location that provides adequate protection from corrosion, considering factors such as exposure to chemicals, moisture, and extreme temperatures.\n",
       "5. **Drainage and Flooding**: Ensure that the location has adequate drainage to prevent flooding and ensure safe operation of the exchanger.\n",
       "6. **Fire Safety and Sprinkler Systems**: Locate exchangers in an area that is not directly adjacent to fire zones or areas with sprinkler systems, while also ensuring that emergency shutdown procedures are easily accessible.\n",
       "7. **Electrical and Instrumentation Access**: Choose a location that provides convenient access to electrical power supplies and instrumentation for monitoring and control of the exchanger.\n",
       "8. **Cable Management and Routing**: Consider the routing of cables and piping to minimize interference and ensure efficient operation of the facility.\n",
       "9. **Noise and Vibration**: Select a location that minimizes noise and vibration from adjacent equipment or operations, ensuring a safe working environment.\n",
       "10. **Regulatory Compliance**: Ensure that the chosen location complies with relevant local, national, and international regulations and standards.\n",
       "\n",
       "By carefully considering these factors, plant designers and operators can optimize the layout of exchangers to achieve efficient, safe, and reliable operation within the plant."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 10\n",
    "chat_with_pdf(\"What are the significant factors that shall be accounted while locating exchangers in a plant layout?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: Clean up when done \n",
    "# vector_db.delete_collection()\n",
    "# print(\"Vector database deleted successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
